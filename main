#Quantitative_Risk


install.packages("qrmdata")
library(qrmdata)

data(SP500)
head(SP500,3)
tail(SP500,3)

plot(SP500)

data("FTSE") # UK FTSE (Financial Times Stock Exchange) index

#a certain date range
ftse00 <- FTSE["2000-04-01/2000-06-30"]

# Load DJ index
data(DJ)

# Show head() and tail() of DJ index
head(DJ)
tail(DJ)

# Plot DJ index
plot(DJ)

# Extract 2008-2009 and assign to dj0809
dj0809 <- DJ["2008/2009"]

# Plot dj0809
plot(dj0809)

# Load DJ constituents data
data(DJ_const)

# Apply names() and head() to DJ_const
names(DJ_const)
head(DJ_const)

# Extract AAPL and GS in 2008-09 and assign to stocks
stocks <- DJ_const["2008/2009", c("AAPL", "GS")]

# Plot stocks with plot.zoo()
plot.zoo(stocks)

# Load exchange rate data
data("GBP_USD")
data("EUR_USD")

# Plot the two exchange rates
plot(GBP_USD)
plot(EUR_USD)

# Plot a USD_GBP exchange rate
plot(1/GBP_USD) #plot the inverse or reciprocal with

# Merge the two exchange rates GBP_USD and EUR_USD
fx <- merge(GBP_USD, EUR_USD, all = TRUE)

# Extract 2010-15 data from fx and assign to fx0015
fx0015 <- fx["2010/2015", ]

# Plot the exchange rates in fx0015
plot.zoo(fx0015)

#log-return calculation 
sp500x <- diff(log(SP500))

head(sp500x, n=3)


sp500x <- diff(log(SP500))[-1]

head(sp500x)

plot(sp500x)



# Compute the log-returns of dj0809 and assign to dj0809_x
dj0809_x <- diff(log(dj0809))[-1]

# Plot the log-returns
plot(dj0809_x)

# Compute the log-returns of djstocks and assign to djstocks_x
djstocks_x <- diff(log(djstocks))[-1]

# Plot the two share returns
plot(djstocks_x)

# Compute the log-returns of GBP_USD and assign to erate_x
erate_x <- diff(log(GBP_USD))[-1]

# Plot the log-returns
plot(erate_x)

# Plot djstocks in four separate plots
plot.zoo(djstocks)

# Plot djstocks in one plot and add legend
plot.zoo(djstocks, plot.type = "single", col=1:4)
legend(julian(x = as.Date("2009-01-01")), y = 70, legend = names(DJ_const)[1:4], fill = 1:4)

# Compute log-returns and assign to djstocks_x
djstocks_x <- diff(log(djstocks))

# Plot djstocks_x in four separate plots
plot.zoo(djstocks_x)

# Plot djstocks_x with vertical bars
plot.zoo(djstocks_x, type="h")


#aggregate log-returns:
sp500x_w <- apply.weekly(sp500x, sum)
sp500x_m <- apply.monthly(sp500x, sum)

#if you have a multivariate time series containing, 
#for example, multiple stock prices, you have to apply the function colSums() instead of sum()

# Plot djx
plot(djx)

# Plot weekly log-returns of djx
plot(apply.weekly(djx,sum),type="h")

# Plot monthly log-returns of djx
plot(apply.monthly(djx,sum),type="h")

# Plot djreturns
plot.zoo(djreturns)

# Plot monthly log-returns of djreturns
plot.zoo(apply.monthly(djreturns,colSums),type="h")

# Plot gold and oil prices
plot(gold)
plot(oil)

# Calculate daily log-returns
goldx <- diff(log(gold))
oilx <- diff(log(oil))

# Calculate monthly log-returns
goldx_m <- apply.monthly(goldx, sum)
oilx_m <- apply.monthly(oilx, sum)

# Merge goldx_m and oilx_m into coms
coms <- merge(goldx_m, oilx_m)

# Plot coms with vertical bars
plot.zoo(coms, type="h")

# Make a pairwise scatterplot of coms
pairs(as.zoo(coms))



#The object zcb contains daily values of Canadian zero-coupon-bond yields,
#expressed as percentages, for the period 2006-2015.

#vector yield_cols containing the names of the columns corresponding
#to maturities of 1, 5 and 10 years has been created. A numerical vector 
#maturity containing all the maturities in years has also been created

# Compute log-returns as zcb_x and simple returns as zcb_x2
zcb_x <- diff(log(zcb))
zcb_x2 <- diff(zcb)


#the head of the 5-year maturity zero coupons
head(zcb[,"5.00y"])

# Plot zcb_x for 1, 5 and 10-year maturities
plot.zoo(zcb_x[, yield_cols])
#or
plot.zoo(cbind(zcb_x[,"1.00y"],zcb_x[,"5.00y"],zcb_x[,"10.00y"]))

# Plot zcb_x2 for 1, 5 and 10-year maturities
plot.zoo(zcb_x2[, yield_cols])

# Plot the yield curve for the first day of zcb
plot(maturity, zcb[1, ], ylim = range(zcb), type = "l", ylab = "yield (%)", col = "red")

#FTSE Example

head(ftse)

tail(ftse)

#give the min and maximum values of FTSE

 

#unbiased sample estimators

mu <- mean(ftse)

sigma <- sd(ftse)

c(mu, sigma)

 

#displaying the fitted normal on the histogram

 

hist(ftse, nclass = 20, probability = TRUE)

lines(ftse, dnorm(ftse, mu, sigma), col="red")

 

#dnorm(x, mean, sd) calculates the probability density function

#(PDF) of the data x with the calculated sample mean and standard deviation;

#this is known as the method-of-moments.

 

#Finally, to calculate an estimate of the density of data x, use density(x).

#This creates a so-called kernel-density estimate (KDE) using a non-parametric

#method that makes no assumptions about the underlying distribution.

 

 

# Calculate average and standard deviation of djx

mu <- mean(djx)

sigma <- sd(djx)

 

# Plot histogram of djx

hist(djx, nclass = 20, probability = TRUE)

 

# Add the normal density as a red line to histogram

lines(djx, dnorm(djx, mean = mu, sd = sigma), col = "red")

 

# Plot non-parametric KDE of djx

plot(density(djx))

 

# Add the normal density as red line to KDE

lines(djx, dnorm(djx, mean = mu, sd = sigma), col = "red")

 

 

#Testing the normality


data <- rnorm(1000, mean=3, sd=2)

 

#Q-Q Plot:

qqnorm(data)

qqline(data, col= "red")

 

#data with heavier tails than normal: inverted S shape

#data with lighter tails than normal: S shape

#data with very skewed distribution: curved shape

 

 

# Make a Q-Q plot of djx and add a red line

qqnorm(djx)

qqline(djx, col="red")

# Calculate the length of djx as n

n <- length(djx)

 

# Generate n standard normal variables, make a Q-Q plot, add a red line

x1 <- rnorm(n, 0,1)

qqnorm(x1)

qqline(x1, col="red")

 

# Generate n Student t variables, make a Q-Q plot, add a red line

x2 <- rt(n, df = 4)

qqnorm(x2)

qqline(x2, col="red")

 

# Generate n standard uniform variables, make a Q-Q plot, add red line

x3 <- runif(n)

qqnorm(x3)

qqline(x3, col="red")

 

 

#Skewness and kurtosis tests:

#Jarque-Bera Test:

 

library(moments)

skewness(data) #symmetry

kurtosis(data) #heavy or light tail

#where theoretical normal values are 0 and 3

 
# Calculate skewness and kurtosis of djx
skewness(djx)
kurtosis(djx)

# Carry out a Jarque-Bera test for djx
jarque.test(djx)

# Calculate skewness and kurtosis of djreturns 
s <- apply(djreturns, 2, skewness)
k <- apply(djreturns, 2, kurtosis)

# Plot k against s and add text labels to identify stocks
plot(s, k, type = "n")
text(s, k, names(s), cex = 0.6)

# Carry out Jarque-Bera tests for each constituent in djreturns
apply(djreturns, 2, jarque.test)




# Calculate weekly and monthly log-returns from djx_d
djx_w <- apply.weekly(djx_d, colSums)
djx_m <- apply.monthly(djx_d, colSums)

# Calculate the p-value for each series in djx_d
apply(djx_d, 2, function(v){jarque.test(v)$p.value})

# Calculate the p-value for each series in djx_w
apply(djx_w, 2, function(v){jarque.test(v)$p.value})

# Calculate the p-value for each series in djx_m
apply(djx_m, 2, function(v){jarque.test(v)$p.value})




#Overlapping returns
#When you aggregate series by summing daily log-returns into longer intervals, 
#you analyze a smaller amount of observations. To preserve the quantity of data, 
#you can calculate overlapping returns with the rollapplyr() function; this also 
#creates strong correlations between observations.

# Calculate a 21-day moving sum of djx
djx21 <- rollapplyr(djx, width = 21, FUN = sum)[-(1:20)]

# Calculate a 63-day moving sum of djx
djx63 <- rollapplyr(djx, width = 63, FUN = sum)[-(1:62)]

# Merge the three series and plot
djx2 <- merge(djx, djx21, djx63, all = FALSE)
plot.zoo(djx2)

# Compute the skewness and kurtosis for each series in djx2
apply(djx2, 2, skewness)
apply(djx2, 2, kurtosis)

# Conduct the Jarque-Bera test to each series in djx2
apply(djx2, 2, jarque.test)

########
#Student-t distribution
#Student-t distribution has 3 parameters: nu, mu, and sigma

library(QRM)
tfit <- fit.st(ftse)
tpars<- tfit$par.ests
tpars

nu <- tpars[1]
mu <- tpars[2]
sigma <- tpars[3]

#displaying the fitted Student t distribution

hist(ftse, nclass = 20, probability= T)
lines(ftse, dnorm(ftse, mean=mean(ftse), sd=sd(ftse), col="red"))

yvals <- dt((ftse-mu)/sigma, df=nu)/sigma
lines(ftse,yvals, col="blue")



# Fit a Student t distribution to djx
tfit <- fit.st(djx)

# Define tpars, nu, mu, and sigma
tpars <- tfit$par.ests
nu <- tpars[1]
mu <- tpars[2]
sigma <- tpars[3]

# Plot a histogram of djx
hist(djx, nclass = 20, probability = TRUE, ylim = range(0, 40))

# Compute the fitted t density at the values djx
yvals <- dt((djx - mu)/sigma, df = nu)/sigma

# Superimpose a red line to show the fitted t density
lines(djx, yvals, col = "red")

# Plot the daily log-return series in fx_d
plot.zoo(fx_d)

# Apply the Jarque-Bera test to each of the series in fx_d
apply(fx_d, 2, jarque.test)

# Plot the monthly log-return series in fx_m
plot.zoo(fx_m, type ="h")

# Apply the Jarque-Bera test to each of the series in fx_m
apply(fx_m, 2, jarque.test)

# Fit a Student t distribution to each of the series in fx_m
apply(fx_m, 2, function(v){fit.st(v)$par.ests})





# Plot the interest-rate return series zcbx_m and zcbx2_m
plot.zoo(zcbx_m, type ="h")
plot.zoo(zcbx2_m, type ="h")

# Make Q-Q plots of the 3rd component series of zcbx_m and zcbx2_m
qqnorm(zcbx_m[,3])
qqnorm(zcbx2_m[,3])

# Compute the kurtosis of each series in zcbx_m and zcbx2_m
apply(zcbx_m,2,kurtosis)
apply(zcbx2_m,2,kurtosis)

# Conduct the Jarque-Bera test on each series in zcbx_m and zcbx2_m
apply(zcbx_m,2,jarque.test)
apply(zcbx2_m,2,jarque.test)

#goldx_q contains quarterly log-returns of the gold price from the 
#beginning of 1990 to the end of 2015.

#Test the data for normality using the Jarque-Bera test, then fit
#a Student t distribution and find the estimated degree of freedom 

jaque.test(goldx_q)

fit.st(goldx_q)

# Mostly the financial data have heavier tail than std. normal.
# Random walk models ~ IID ?
# if so, past prices can not tell anything about the future prices. 
#The best guest for the future price, therefore, could be today's price.
# Real returns often show volatility clustering


#djx contains the Dow Jones index
#npars and tpars contain the parameter estimates that are obtained
#when a normal distribution and a t distribution are fitted to djx


# Compute the length n of djx 
n <- length(djx)

#  Generate a normal sample of size n with parameters given by npars
ndata <- rnorm(n,mean=0,sd=1)*npars[2] + npars[1]

# Generate a t-distributed sample of size n with paramaters given by tpars
tdata <- rt(n, df = tpars[1])*tpars[3] + tpars[2]

# Make ndata and tdata into xts objects
ndatax <- xts(ndata, time(djx))
tdatax <- xts(tdata, time(djx))

# Merge djx, ndatax, and tdatax and plot
alldata <- merge(djx,ndatax,tdatax)
plot.zoo(alldata, type = "h", ylim = range(alldata))

#when there is volatility clustering, and you know you are in the middle of 
#a particularly volatile period, then you know that the probability of another 
#extreme market move is increased.

# If we know that there is volatility clustering, then serial correlations are 
#specifically important


#Thus, estimating the sample autocorrelation is significant.

#When you estimate sample autocorrelations, you are making the implicit assumption
#that the series has a property known as stationarity. Consider a daily log-return
#series. Stationarity would mean that the expected daily return is always the same;
#the variance of daily returns is always the same, and the correlation between 
#returns on days the same distance apart is always the same.


# remember, the sign change in time series is very significant. Thus, it is better 
# to use absolute value in correlogram to check acf
acf(ftse)
acf(abs(ftse))



# Set up a plot region to show 3 plots at a time
par(mfrow = c(3, 1))

# Plot the acfs of djx, ndata and tdata
plot(djx)
plot(ndata)
plot(tdata)

# Plot the acfs of the absolute values
par(mfrow = c(3, 1))
acf(abs(djx))
acf(abs(ndata))
acf(abs(tdata))

# Plot the acfs of the squares of the values
par(mfrow = c(3, 1))
acf((djx)^2)
acf((ndata)^2)
acf((tdata)^2)


# The Ljung-Box Test
# testing the iid hypothesis

Box.test(ftse, lag=10, type="Ljung")

# Apply the Ljung-Box test to djx
Box.test(djx, lag = 10, type = "Ljung")

# Apply the Ljung-Box test to absolute values of djx
Box.test(abs(djx), lag = 10, type = "Ljung")

# Apply the Ljung-Box test to all return series in djall
apply(djall, 2, Box.test, lag = 10, type = "Ljung")

# Apply the Ljung-Box test to absolute values of all returns in djall
apply(abs(djall), 2, Box.test, lag = 10, type = "Ljung")



# Create monthly log-returns from djx
djx_m <- apply.monthly(djx, sum)

# Apply Ljung-Box tests to raw and absolute values of djx_m
Box.test(djx_m, lag = 10, type = "Ljung")
Box.test(abs(djx_m), lag = 10, type = "Ljung")

# Create monthly log-returns from djall
djall_m <- apply.monthly(djall, FUN= colSums)

# Apply Ljung-Box tests to raw and absolute values of djall_m
apply(djall_m, 2, Box.test, lag = 10, type = "Ljung")
apply(abs(djall_m), 2, Box.test, lag = 10, type = "Ljung")

#The amount of serial dependence appears to decrease when we move from daily to monthly returns.


#Extracting the extreme of return series

ftse <- diff(log(FTSE))["1991-01-02/2010-12-31"]
ftse_losses <- ftse
ftse_extremes <- ftse_losses[ftse_losses > 0.025]

head(ftse_extremes)

length(ftse_extremes) #how many extreme value we have

plot(ftse_extremes, type= "h", auto.grid=F)
#the pattern show the characteristic of iid or serial correlation



# Partition plotting area into 3 pieces
par(mfrow = c(1, 3))

# Plot djx_extremes
plot(djx_extremes, type = "h")

# Compute the spaces between the times of the extremes
djx_spaces <- diff(time(djx_extremes))

# Make a histogram of these spaces
hist(as.numeric(djx_spaces))

# Make a Q-Q plot of djx_spaces against exp_quantiles
qqplot(exp_quantiles, djx_spaces)

# Carry out the previous 4 steps for iid_extremes
plot(iid_extremes, type = "h")
iid_spaces <- diff(time(iid_extremes))
hist(as.numeric(iid_spaces))
qqplot(exp_quantiles, iid_spaces)



# Plot fx and fx_w
plot.zoo(fx, type="h")
plot.zoo(fx_w, type="h")

# Make acf plots of fx and the absolute values of fx
acf(fx)
acf(abs(fx))

# Apply the Ljung-Box test to the components of fx and their absolute values
apply(fx, 2, Box.test, lag = 10, type = "Ljung")
apply(abs(fx), 2, Box.test, lag = 10, type = "Ljung")

# Make acf plots of fx_w and the absolute values of fx_w
acf(fx_w)
acf(abs(fx_w))


# Apply the Ljung-Box test to the components of fx_w and their absolute values
apply(fx_w, 2, Box.test, lag = 10, type = "Ljung")
apply(abs(fx_w), 2, Box.test, lag = 10, type = "Ljung")





#volatility and serial dependence is also a feature of daily and monthly interest-rate log-returns.
#The dataset zcb_x contains daily log-returns for the 1-year, 5-year and 10-year Canadian 
#zero-coupon bond yields while zcbx_m contains the corresponding monthly log-returns.




# Make acf plots of zcb_x and the absolute values of zcb_x
acf(zcb_x)
acf(abs(zcb_x))

# Apply the Ljung-Box test to the components of zcb_x and their absolute values
apply(zcb_x,2, Box.test, lag=10, type ="Ljung")
apply(abs(zcb_x),2, Box.test, lag=10, type ="Ljung")

# Make acf plots of zcbx_m and the absolute values of zcbx_m
acf(zcbx_m)
acf(abs(zcbx_m))


# Apply the Ljung-Box test to the components of zcbx_m and their absolute values
apply(zcbx_m,2, Box.test, lag=10, type ="Ljung")
apply(abs(zcbx_m),2, Box.test, lag=10, type ="Ljung")



#International Equity Portfolio
#If an investor invests her wealth in 30 % FTSE, 40% S&P500, and 30% SMI
#Risk Factors: FTSE, S&P500, SMI, GBP/USD,and GBP/CHP

riskfactors <- merge(FTSE, SP500, SMI, USD_GBP, CHP_GBP, all=F)["/2012-12-31",]
plot.zoo(riskfactors)

#Historical Simulation
#non-parametric method: resample historical risk-factor returns and examine their effect on current portfolio
#Loss operator shows effect of different risk-factor returns on the portfolio.

#VaR 
#simulaate a loss
losses <- rnorm(100)
losses_o <- sort(losses, decreasing =T) #ordered
head(losses_o)

quantile(losses_o, 0.95) # VaR for 95%

#compare with the exact value
qnorm(0.95) #since it is just quantile

#Expected shortfall (ES)
mean(losses[losses >quantile(losses, 0.95)])

#check with the written function 
ESnorm(0.95)


# Plot the risk-factor data
plot.zoo(riskfactors)

# Calculate the log-returns, assign to returns, and plot
returns <- diff(log(riskfactors))[-1, ]
plot.zoo(returns)

# Use apply() to carry out the Jarque-Bera test for all 5 series
apply(returns, 2, jarque.test)

# Make a Q-Q plot against normal for the 5th return series and add a reference line
qqnorm(returns[, 5])
qqline(returns[, 5])

# Make a picture of the sample acfs for returns and their absolute values
acf(returns)
acf(abs(returns))


#The function lossop() is the so-called loss operator for the portfolio 
#and has been specially written for this exercise. In general, for each new portfolio, 
#a specific function has to be written to compute portfolio losses and gains.

# Calculate the loss from a log-return of -0.1 for all risk factors
lossop(rep(-0.1, 5))

# Apply lossop() to returns and plot hslosses
hslosses <- lossop(returns)
plot(hslosses)

# Form a Q-Q plot of hslosses against normal
qqnorm(hslosses)
qqline(hslosses)

# Plot the sample acf of hslosses and their absolute values
acf(hslosses)
acf(abs(hslosses))



# Estimate the 99th sample percentile of the distribution of hslosses
quantile(hslosses, 0.99)

# Estimate the 99% ES
mean(hslosses[hslosses >= quantile(hslosses, 0.99)])

# Estimate the mean and standard deviation of hslosses
mu <- mean(hslosses)
sigma <- sd(hslosses)

# Compute the 99% quantile of a normal distribution
qnorm(0.99, mean = mu, sd = sigma)

# Compute the 99% ES of a normal distribution
ESnorm(0.99, mu = mu, sd = sigma)


#########################
#Option Portfolio and Black Scholes

#Pricing afirst call option:

K <- 50 #strike
T <- 2
t <- 0 #current time
S <- 40 #current time price
r <- 0.005  #interest rate
sigma <- 0.25 # volatility
Black_Scholes(t, S, r, sigma, K, T, "call")


Black_Scholes(t, S, r, sigma*1.2, K, T, "call") #if the volatility increase by 20%
# the price of the option is higher




